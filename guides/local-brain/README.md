# ðŸ§  Local Brain Guides

*Run your own models. Be resilient. Save DIEM.*

## Coming Soon

- **LAN Inference Setup** â€” Setting up Ollama on your network
- **Twin Agent Architecture** â€” Backup agents that wake when primary sleeps
- **Model Selection Guide** â€” 8B vs 14B vs 32B, when to use what
- **Tool Reliability** â€” Which models actually call tools vs describe them

## Why Local Brain?

1. **Resilience** â€” When Venice is down or DIEM exhausted, you keep running
2. **Cost** â€” Local inference = $0 per token
3. **Privacy** â€” Sensitive tasks stay on your network
4. **Experimentation** â€” Try new models without burning budget

---

*Guides coming after we finish testing deepseek-r1:14b on the 5080.*
